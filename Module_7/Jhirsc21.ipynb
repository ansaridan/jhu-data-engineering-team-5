{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7 - Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will make all the `matplotlib` images appear in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directions\n",
    "\n",
    "**Failure to follow the directions will result in a \"0\"**\n",
    "\n",
    "The due dates for each are indicated in the Syllabus and the course calendar. If anything is unclear, please email EN605.448@gmail.com the official email for the course or ask questions in the Lab discussion area on Blackboard.\n",
    "\n",
    "The Labs also present technical material that augments the lectures and \"book\".  You should read through the entire lab at the start of each module.\n",
    "\n",
    "### General Instructions\n",
    "\n",
    "1.  You will be submitting your assignment to Blackboard. If there are no accompanying files, you should submit *only* your notebook and it should be named using *only* your JHED id: fsmith79.ipynb for example if your JHED id were \"fsmith79\". If the assignment requires additional files, you should name the *folder/directory* your JHED id and put all items in that folder/directory, ZIP it up (only ZIP...no other compression), and submit it to Blackboard.\n",
    "    \n",
    "    * do **not** use absolute paths in your notebooks. All resources should appear in the same directory as the rest of your assignments.\n",
    "    * the directory **must** be named your JHED id and **only** your JHED id.\n",
    "    \n",
    "2. Data Science is as much about what you write (communicating) as the code you execute (researching). In many places, you will be required to execute code and discuss both the purpose and the result. Additionally, Data Science is about reproducibility and transparency. This includes good communication with your team and possibly with yourself. Therefore, you must show **all** work.\n",
    "\n",
    "3. Avail yourself of the Markdown/Codecell nature of the notebook. If you don't know about Markdown, look it up. Your notebooks should not look like ransom notes. Don't make everything bold. Clearly indicate what question you are answering.\n",
    "\n",
    "4. Submit a cleanly executed notebook. The first code cell should say `In [1]` and each successive code cell should increase by 1 throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference\n",
    "\n",
    "Really, there are only a few classical problems in statistical inference. However, we use the Bayes Theorem as the basis for solving all of them:\n",
    "\n",
    "$$P(H|D) = \\frac{P(D|H)P(H)}{P(D)}$$\n",
    "\n",
    "You only need to identify what $H$ relates to...what is it? Is it some parameter of a distribution? Some property of a model (coefficients, error rate, etc.). For some formulations, we are more specific and specify $H$ as some parameter or parameters, $\\theta$:\n",
    "\n",
    "$$P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}$$\n",
    "\n",
    "In the text we saw how we could estimate the posterior distribution using four methods: Grid, Exact, Monte Carlo and Bootstrap. For this Lab, we'll concentrate on the Bootstrap method for the reasons specified in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical inference of a proportion in a Bernoulli Trial\n",
    "\n",
    "**1\\. Suppose we have a coin that shows up heads 60% of the time ($\\theta=p=0.6$). Generate 100 observations from this Binomial distribution (either as True/False or 1/0).** (This is just one way of generating synthetic data but a typical one: pick parameters for a distribution and then generate values from that distribution at random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed([1244875])\n",
    "\n",
    "theta = 0.6\n",
    "data = [1 if np.random.rand() < theta else 0 for _ in range( 100)]\n",
    "print( data[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the synthetic data. At this point, we pretend that this is data we collected from the real world and we have no idea what $\\theta$ really is.\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Note</strong>\n",
    "    <p>This messes people up all the time. There is a difference between:</p>\n",
    "    <p><tt>data = [1 if np.random.rand() &lt; 0.6 else 0 for _ in range( 100)]</tt></p>\n",
    "    <p>which is generating random data (or \"synthetic data\" or \"pseudo data\" or \"fake data\") based on a desired unobserved parameter and...</p>\n",
    "    <p>recreating a data set (or variable) of boolean outcomes which would be:</p>\n",
    "    <p><tt>data = [1] * 60 + [0] * 40</tt></p>\n",
    "    <p><strong>be very, very careful that you're using the right one at the right time. If you just need to create some random data, you can use the first. If you need to re-create an actual data set or create a data set with a precise parameterization, use the second.</strong></p>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Understanding that inference is not certain, our goal is to make inferences about this parameter's value using this data we just \"collected.\" Normally, the first thing we do is just calculate the parameter from our data. An *estimate* of some real world parameter is often given a \"hat\", for example $\\theta$ becomes $\\hat{\\theta}$. Sometimes, it goes from Greek to Latin as in $\\sigma$ to $s$ and sometimes it gets an adornment as well as in $\\mu$ to $\\bar{x}$.\n",
    "\n",
    "**2\\. Calculate $\\hat{theta}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n"
     ]
    }
   ],
   "source": [
    "theta_est = np.mean( data)\n",
    "print( theta_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we know that this $\\hat{\\theta}$ is not necessarily the \"true\" value. We want to know all the values that are supported by the data we collected and the degree to which they are supported...how confident we are in them. This is basically what we get when we calculate a posterior distribution over $\\theta$ based on the data.\n",
    "\n",
    "And this is where the **(Non-Parameteric Bayesian) Bootstrap** estimate of that posterior distribution comes in. In the text we established *theoretically* how we went from a single data set to an estimate of the posterior distribution of our parameters. Now we're going to do it for reals.  Use the data we have to \"bootstrap\" an estimate of the posterior probability distribution over $\\theta$, $P(\\theta|D)$ which is \"given the data we observed, how much are we to believe in the various values of $\\theta$ and how much should we believe in them?\" Remember that belief is quantified as probability.\n",
    "\n",
    "**3\\. Generate the Bootstrap of the posterior distribution of $\\hat{\\theta}$ and answer the following questions:**\n",
    "\n",
    "First, we write a simple function to do our bootstrap sampling for us. It takes the data, a metric function and the number of bootstrap samples as the arguments. A metric function can be anything we like but it will most likely be something like `np.mean`, `np.var`, etc., it is whatever function we use to calculate our parameter/statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample( data, f, n=100):\n",
    "    result = []\n",
    "    m = len( data)\n",
    "    for _ in range( n):\n",
    "        sample = np.random.choice( data, len(data), replace=True)\n",
    "        r = f( sample)\n",
    "        result.append( r)\n",
    "    return np.array( result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we used the function by supplying the data we \"collected\", our metric function `np.mean` and indicate we want 1000 bootstrap samples. This returns the data we can use as our posterior distribution of the proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = bootstrap_sample( data, np.mean, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we like, we can plot a histogram of this posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(10, 6)) # first element is width, second is height.\n",
    "\n",
    "axes = figure.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.hist( posterior, density=True)\n",
    "axes.set_ylabel( \"Density\")\n",
    "axes.set_xlabel( \"$\\hat{theta}$\")\n",
    "axes.set_title( \"Posterior Distribution of $\\hat{theta}$\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while the data is discrete and boolean (true/false), the parameter $\\theta$ is continuous. You might also notice that our distribution appears to be normally distributed. Based on the Central Limit Theorem, this is what we'd expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4\\. What is the 90% Credible Interval (Bayesian Confidence Interval) for $\\hat{\\theta}$? Interpret it.**\n",
    "\n",
    "Although we'll often plot the posterior distribution, the real payoff from having it is to be able to do computations with it. There are a number of functions we can use for that purpose, for example, `mquantiles`. `mquantiles` is normally used to summarize the distributions of data but in this case, our data is estimates of $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59, 0.74])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mstats.mquantiles( posterior, [0.05, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important part of Data Science and assignments in this course is interpreting the results. This is not purely a coding class. Therefore, you should always, *always* interpret your results:\n",
    "\n",
    "There is a 90% probability that the value of $\\theta$ is between 0.59 and 0.74 based on the data.\n",
    "\n",
    "Of course, there's nothing magical about only looking at the 90% confidence/credible interval and you can look at other ranges of interest as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5\\. In Bayesian Statistics, we often identify a range of possible values for a parameter that we consider the same. This is known as the ROPE (Region of Practical Equivalance). We know that a fair coin would have $\\theta$ of 0.5 but we're unlikely to get an exact value of 0.5 from our data. If the ROPE is 0.48-0.52, what is the probability that our coin's $\\theta$ lies in that range and is thus \"fair\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((0.48 <= posterior) & (posterior <= 0.52))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the downsides to the Boostrap approach is that we do not follow \"Cromwell's Dictum\" and we can get events with zero probability. We should just interpret these events are really have very small probabilities.\n",
    "\n",
    "Of course, now that we have this posterior distribution we can answer all kinds of (possibly) interesting and relevant questions to our problem. Let's stick with the basics, for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "**Exercise 1.**\n",
    "\n",
    "**Learning Objective: apply the Bootstrap to a realistic problem. For additional examples, refer to the Applications section of the Inference chapter in *Fundamentals*.**\n",
    "\n",
    "In addition to estimates of the posterior distribution of parameters such as $\\theta$, we are often interested in the posterior distribution of the *difference* of two $\\theta$s. For example, we might be interested in the *difference* between the proportion of men who smoke ($\\theta_{men}$) and the proportion of women who smoke ($\\theta_{women}$). Using the Non-Parametric Bootstrap, we can generate posterior distributions for $\\hat{\\theta}_{men}$ and $\\hat{\\theta}_{women}$ as well as $d$, the *difference*.\n",
    "\n",
    "These are the steps:\n",
    "\n",
    "1. In the example above, we made up the data. However, if $\\theta_{men}$ = 0.23 and $\\theta_{women}$ = 0.34 and there were 100 observations each, we can re-create the actual samples. Re-create the actual samples.\n",
    "2. Generate the bootstrap samples for each group.\n",
    "3. Generate difference data. You can do this by simply subtracting, element by element, one bootstrap sample from the other, $\\theta_{men}$ - $\\theta_{women}$. (Why?)\n",
    "4. Plot the distributions of all three.\n",
    "5. Calculate the 90% Bayesian Confidence Interval of all three **and interpret them**.\n",
    "6. Determine a ROPE for the difference and tell me what's the probability that the \"true\" value of the difference falls in the ROPE.\n",
    "\n",
    "Use as many Markdown Cells and Code Cells as you need; it should look nice (not like a ransom note)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of using bootstrap sampling to estimate a posteriod distribution will stay with us throughout the entire semester. This will be our fundamental approach to statistical inference (there are other approaches and there are other *Bayesian* approaches). The important thing is to understand 1. why and 2. the dimensions along which the problems can vary such as,\n",
    "\n",
    "1. The nature of data. The data may take on a variety of different types. We've looked primarily at boolean or Bernoulli data. However, the data might be categorical (more than two discrete outcomes), counts, real valued, etc. This means that there may be more than one $\\theta$. For example, the normal distribution has two $\\theta$s: the mean, $\\mu$, and the variance, $\\sigma^2$. But you should think even more broadly than this. A linear regression as many $\\theta$s: the coefficients, the coefficient of determination, the error of the regression, etc. A decision tree has a structure and error rate.\n",
    "2. A related concept is variability. We may have two true values, 0.23 and 0.24, but the variability of the data may not permit us to distinguish between them.\n",
    "3. Another dimension is the amount of data. We may not be able to get a \"good\" inference because we have not collected enough data.\n",
    "\n",
    "And, of course, all of these will and do interact. And a lot of experimental design is based on trying to limit variability (by \"holding other things constant\") and to get the \"right\" amount of data to support the inference we want to make.\n",
    "\n",
    "These exercises investigate some of the dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "**Learning Objective: peek under the covers of the Bootstrap and see how different factors influence our inferences.**\n",
    "\n",
    "**1\\. Repeat the guided example (coin flips) with a $\\theta = 0.05$ and discuss. Were the credible intervals the same size? Was your estimate of $\\theta$ as good? What does this say about statistical inference on relatively rare events or extreme values?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical Inference for a single real valued $\\theta$**\n",
    "\n",
    "**Exercise 3**\n",
    "\n",
    "We can do the same thing for a real valued data (like weights, heights, etc.) and the $\\theta$'s that describe such distributions. If we have a normal distribution, there are two such $\\theta$s, $\\mu$, the mean, and $\\sigma$, the standard deviation. Remember, however, that we often think of the dispersion of our data as a percent of the mean or the *coefficient of variation*, v, the standard deviation over the mean (\"variation as a percent of the mean\").\n",
    "\n",
    "$$v = s/\\bar{x}$$\n",
    "\n",
    "for the following problems, you're going to think in terms of $v$ and solve for the right $s$ to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a\\. Generate 50 observations from a normal distribution with $\\mu=102.7$ and $v=5\\%$.**\n",
    "\n",
    "You should refer to the previous Lab for generating synthetic data from the normal distribution and working with $v$, the coefficient of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed([2386431651])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. What is $\\bar{x}$?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Generate the Bootstrap estimate of the posterior distribution of $\\bar{x}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d. What is the 90% Credible Interval (Bayesian Confidence Interval) for $\\hat{\\theta}$?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e. Define a ROPE of \"about 100\". What is the probability that $\\bar{x}$ falls within the ROPE?.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4\\. Repeat Steps #1-5 with $v=25\\%$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed([484716248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5\\. Repeat Steps #1-5 with $v=25\\%$ and 500 samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed([484716248])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical Inference for a two real valued $\\theta$s**\n",
    "\n",
    "**Exercise 6. Following *Fundamentals*, apply the Bootstrap to make inferences about the difference between two means.**\n",
    "\n",
    "Below are the Quantiative GRE scores for STEM and Social Science graduates respectively. Analyze the difference in mean scores. \n",
    "\n",
    "Start by formulating a ROPE for this problem. Be clever but but don't overdo it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_gre = [164.0, 168.0, 165.0, 171.0, 171.0, 156.0, 170.0, 159.0, 166.0, 155.0, 166.0, 159.0, 170.0, 157.0, 167.0, 156.0, 155.0, 160.0, 174.0, 158.0, 168.0, 174.0, 162.0, 161.0, 167.0, 159.0, 152.0, 163.0, 164.0, 171.0, 156.0, 165.0, 165.0, 175.0, 152.0, 172.0, 162.0, 168.0, 173.0, 167.0, 158.0, 168.0, 158.0, 167.0, 159.0, 168.0, 157.0, 158.0, 166.0, 163.0, 158.0, 165.0, 155.0, 160.0, 178.0, 165.0, 161.0, 165.0, 162.0, 165.0, 170.0, 162.0, 165.0, 164.0, 175.0, 155.0, 161.0, 161.0, 165.0, 168.0, 165.0, 171.0, 175.0, 161.0, 161.0, 167.0, 155.0, 160.0, 168.0, 159.0, 171.0, 150.0, 164.0, 167.0, 165.0, 153.0, 170.0, 170.0, 164.0, 160.0, 161.0, 169.0, 163.0, 162.0, 173.0, 161.0, 162.0, 167.0, 169.0, 155.0]\n",
    "soc_sci_gre = [157.0, 162.0, 164.0, 152.0, 170.0, 144.0, 142.0, 145.0, 150.0, 157.0, 154.0, 149.0, 169.0, 163.0, 163.0, 159.0, 150.0, 156.0, 157.0, 168.0, 149.0, 162.0, 156.0, 164.0, 152.0, 155.0, 156.0, 161.0, 144.0, 151.0, 159.0, 159.0, 150.0, 146.0, 172.0, 167.0, 162.0, 137.0, 164.0, 157.0, 150.0, 146.0, 167.0, 159.0, 152.0, 156.0, 154.0, 163.0, 167.0, 154.0, 151.0, 150.0, 162.0, 148.0, 155.0, 147.0, 160.0, 164.0, 162.0, 161.0, 146.0, 135.0, 143.0, 159.0, 163.0, 165.0, 175.0, 163.0, 156.0, 159.0, 160.0, 155.0, 137.0, 137.0, 156.0, 160.0, 153.0, 151.0, 164.0, 147.0, 152.0, 146.0, 138.0, 149.0, 149.0, 153.0, 154.0, 160.0, 149.0, 168.0, 133.0, 156.0, 166.0, 161.0, 154.0, 163.0, 162.0, 147.0, 157.0, 162.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "1\\. Discuss the similarities and differences in your results for Exercises 3-5. What do you think caused them given they all have the same mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Why are we interested in estimating the posterior distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. In the previous Lab, we talked about how Systems Theory related to the variability of a variable. How then is \"keeping other things the same\" in experimental design or comparison related both to inference and Systems Theory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Your Own\n",
    "\n",
    "We have only scratched the surface here. What you really want to understand is how variability and the amount of data you have interact especially when looking at *differences* in proportions and means. \n",
    "\n",
    "Based on the experiments above, two things tend to happen. First, the bounds of the Credible/Confidence Interval can change. They can get bigger or smaller. And they can contain the \"true\" value or not or with lesser or greater probility.\n",
    "\n",
    "Second, the probability of the ROPE changes. Additionally, the probability that a value of interest is contained in the ROPE changes.\n",
    "\n",
    "What you want to see, under controlled circumstances, is how the sample size and dispersion of the data interact to affect your conclusions.\n",
    "\n",
    "To do this, you could take examples above and,\n",
    "\n",
    "1. decrease $v$.\n",
    "2. increase $v$.\n",
    "3. decrease observations.\n",
    "4. increase observations,\n",
    "5. change the difference in the real $\\theta$s both for normal ($\\mu$) and bernoulli distributions keeping the other factors fixed to see what differences are and are not detectable with those factors (variability and data).\n",
    "6. change the ROPE...for example, supposed we *did* believe the mean was \"around 102\". How would these experiments affect you conclusions.\n",
    "7. do the same experiment over with a different random seed!\n",
    "\n",
    "You can write a helper function that does all the things at once to more quickly see what's going on. Additionally, make hypotheses ahead of time about what you think will happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your work here*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "171px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
